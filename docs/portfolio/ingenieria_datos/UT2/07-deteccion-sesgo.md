---
title: "âš–ï¸ Sesgo y Fairness â€” Boston, Titanic y Ames"
date: 2025-10-12
---

# âš–ï¸ Sesgo y Fairness â€” Boston, Titanic y Ames

# ğŸŒ Contexto

Esta prÃ¡ctica pertenece a la **Unidad TemÃ¡tica 2: Calidad y Ã‰tica de los Datos**, centrada en el anÃ¡lisis de **sesgo y equidad algorÃ­tmica** en distintos contextos.  
El objetivo fue comprender cÃ³mo los datos reflejan desigualdades estructurales y cÃ³mo medir, mitigar y documentar el impacto de decisiones algorÃ­tmicas sobre grupos sensibles.

Se trabajÃ³ con tres datasets clÃ¡sicos:
1. **Boston Housing** â†’ regresiÃ³n y sesgo racial histÃ³rico.  
2. **Titanic** â†’ clasificaciÃ³n y sesgo sistemÃ¡tico por gÃ©nero y clase social.  
3. **Ames Housing** â†’ regresiÃ³n con brechas geogrÃ¡ficas y temporales.  

El anÃ¡lisis combinÃ³ mÃ©tricas de *fairness*, evaluaciones interseccionales y mitigaciÃ³n con **Fairlearn**, destacando los *trade-offs* entre equidad y performance.

---

# ğŸ¯ Objetivos

- Detectar **brechas y sesgos sistemÃ¡ticos** en distintos contextos de predicciÃ³n.  
- Medir la equidad mediante mÃ©tricas como **Demographic Parity** y **Equalized Odds**.  
- Implementar **Fairlearn (ExponentiatedGradient)** para mitigar desigualdades.  
- Cuantificar el **performance loss** tras la mitigaciÃ³n.  
- Reflexionar sobre las **implicancias Ã©ticas y sociales** del sesgo en modelos predictivos.

---

# ğŸ“¦ Datasets

| Dataset | Tipo | Grupo sensible | Ejemplo de sesgo analizado |
|----------|------|----------------|-----------------------------|
| **Boston Housing** | RegresiÃ³n | Variable racial (Bk_racial / B) | Diferencias estructurales histÃ³ricas |
| **Titanic** | ClasificaciÃ³n | Sexo Ã— Clase | Probabilidad de supervivencia desigual |
| **Ames Housing** | RegresiÃ³n | Barrio / AÃ±o de construcciÃ³n | Brechas geogrÃ¡ficas y temporales |

---

# ğŸ§¹ Limpieza y preparaciÃ³n de datos

Cada dataset fue preparado para su anÃ¡lisis:

1. **Boston:** se eliminÃ³ la variable sensible del modelo, pero se evaluaron brechas en los residuales.  
2. **Titanic:** se estandarizaron categorÃ­as (`sex`, `pclass`) y se generÃ³ la intersecciÃ³n `sexo Ã— clase`.  
3. **Ames:** se agruparon propiedades por `Neighborhood` y `YearBuilt` para medir disparidades.

El pipeline de fairness incluyÃ³ pasos de preprocesamiento, modelado base, cÃ¡lculo de mÃ©tricas y mitigaciÃ³n.

---

# âš™ï¸ AnÃ¡lisis tÃ©cnico

## ğŸ”¹ 1) Boston Housing â€” RegresiÃ³n y sesgo racial

El dataset contiene correlaciones histÃ³ricas entre raza y precio de vivienda.  
Se cuantificÃ³ la brecha promedio en las predicciones por grupo racial.

ğŸ“Š **Resultado:**  
> Diferencia de predicciÃ³n media â‰ˆ **âˆ’2.4%** (precio menor para zonas de poblaciÃ³n no blanca).

ğŸ“ **DecisiÃ³n Ã©tica:**  
Caso **solo educativo**, no apto para uso productivo debido a su sesgo estructural.

---

## ğŸ”¹ 2) Titanic â€” ClasificaciÃ³n, gÃ©nero y clase

Se entrenÃ³ un modelo base y se evaluaron mÃ©tricas de fairness por grupo (`sexo`, `clase`) usando `Fairlearn.MetricFrame`.

```python
from fairlearn.metrics import MetricFrame, selection_rate
from fairlearn.reductions import ExponentiatedGradient, DemographicParity
from sklearn.metrics import accuracy_score, recall_score

mf_sex = MetricFrame(
    metrics={"accuracy": accuracy_score, "selection_rate": selection_rate,
             "tpr": lambda yt, yp: recall_score(yt, yp, zero_division=0)},
    y_true=y_test_t, y_pred=titanic_baseline_pred, sensitive_features=A_test_t
)
mf_sex.by_group  # resumen por sexo
```

ğŸ“ˆ **MÃ©tricas principales:**

| MÃ©trica | Valor | InterpretaciÃ³n |
|----------|--------|----------------|
| Demographic Parity Diff | 0.113 | Brecha de probabilidad de resultado positivo |
| Equalized Odds Diff | 0.240 | Diferencias en verdaderos positivos por grupo |
| Performance loss tras mitigaciÃ³n | 8.3% | DisminuciÃ³n de accuracy para ganar equidad |

ğŸ§  **Interseccionalidad (`sexo Ã— clase`):**
- Peor subgrupo: **male_3** â†’ selection_rate = 0.095, TPR = 0.154  
- Refleja desigualdad combinada de gÃ©nero y estatus socioeconÃ³mico.

---

## ğŸ”¹ 3) Ames Housing â€” RegresiÃ³n con brechas espaciales y temporales

Se analizÃ³ la variaciÃ³n del **MAE** por `Neighborhood` y aÃ±o de construcciÃ³n.  

| Grupo | MÃ©trica | Brecha relativa |
|--------|----------|----------------|
| Barrios caros vs. baratos | MAE | +132% |
| Viviendas nuevas vs. antiguas | MAE | +47% |

ğŸ“Š **ConclusiÃ³n:**  
Las disparidades reflejan diferencias estructurales de acceso y valor territorial, lo que puede amplificar desigualdades si no se ajusta el modelo.

---

# ğŸ“ˆ Evidencias

### ğŸ”¹ Titanic â€” Fairness e interseccionalidad  
- **Demographic Parity Diff (sexo): 0.113**  
- **Equalized Odds Diff (sexo): 0.240**  
- **Peor subgrupo (`sexoÃ—clase`):** male_3 â€” selection_rate=0.095, TPR=0.154.  
- **Trade-off (mitigaciÃ³n):** Performance loss â‰ˆ 8.3%.

### ğŸ”¹ Boston â€” Brecha histÃ³rica  
- **Brecha detectada:** âˆ’2.4% en predicciones medias.

### ğŸ”¹ Ames â€” Disparidades por grupo  
- **Brecha geogrÃ¡fica (MAE):** +132% (barrio mÃ¡s caro vs. mÃ¡s barato).  
- **Brecha temporal (MAE):** +47% (casas nuevas vs. antiguas).

### ğŸ“ [Notebook](../../../notebooks/UT2-3.ipynb)

---

# ğŸ§  Resultados y discusiÃ³n

| Caso | Hallazgo clave | ImplicaciÃ³n Ã©tica |
|------|----------------|------------------|
| **Boston** | Sesgo racial estructural | No desplegar, mantener como caso educativo |
| **Titanic** | Disparidad de oportunidades por gÃ©nero y clase | Mitigar con Fairlearn y documentar pÃ©rdida de rendimiento |
| **Ames** | Diferencias geogrÃ¡ficas y temporales | Auditar antes de aplicar decisiones financieras o de crÃ©dito |

> ğŸ’¬ **DiscusiÃ³n:**  
> La equidad algorÃ­tmica no es un estado binario, sino un proceso continuo.  
> Cada modelo requiere auditorÃ­a, transparencia y responsabilidad sobre cÃ³mo sus predicciones afectan a distintos grupos sociales.

---

# ğŸ”— ConexiÃ³n con otras unidades

- **UT1:** Expande el anÃ¡lisis de fuentes, ahora con foco en los impactos Ã©ticos.  
- **UT3:** Fairness influye directamente en el diseÃ±o de *features* y variables sensibles.  
- **UT5:** Se integrarÃ¡ dentro de pipelines automatizados de evaluaciÃ³n Ã©tica.

---

# ğŸ§© ReflexiÃ³n final

El sesgo en los datos no se elimina, se **reconoce y gestiona**.  
AprendÃ­ que la equidad requiere decisiones conscientes: a veces, un modelo menos preciso puede ser mÃ¡s justo.  
La Ã©tica de datos es una dimensiÃ³n tÃ©cnica, pero tambiÃ©n profundamente humana.

---

# ğŸ§° Stack tÃ©cnico

**Lenguaje:** Python  
**LibrerÃ­as:** Pandas Â· Scikit-learn Â· Fairlearn  
**Conceptos aplicados:** Fairness Â· MitigaciÃ³n Â· MÃ©tricas por grupo Â· Performance loss  

---

# ğŸ“š Referencias

- PrÃ¡ctica: <https://juanfkurucz.com/ucu-id/ut2/07-sesgo-y-fairness/>  
- [Fairlearn Documentation](https://fairlearn.org/)  
- Barocas, S., Hardt, M., & Narayanan, A. (2023). *Fairness and Machine Learning.*  
- [scikit-learn Documentation](https://scikit-learn.org/)