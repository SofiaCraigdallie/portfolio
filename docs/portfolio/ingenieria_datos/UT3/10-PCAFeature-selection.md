---
title: "ğŸ” UT3 Â· PrÃ¡ctica 10 â€” PCA vs Feature Selection en Ames Housing"
date: 2025-11-09
---

# ğŸ” PCA vs Feature Selection en Ames Housing

---

# ğŸŒ Contexto

Esta prÃ¡ctica corresponde a la **Unidad TemÃ¡tica 3: Feature Engineering y SelecciÃ³n** del Portafolio de IngenierÃ­a de Datos.  
Se trabaja con el dataset **Ames Housing** (competencia â€œHouse Pricesâ€ de Kaggle), un clÃ¡sico para regresiÃ³n de precios inmobiliarios.  
El foco es comparar dos caminos para manejar muchas variables: **reducciÃ³n de dimensionalidad con PCA** vs **selecciÃ³n de variables** (*filter, wrapper, embedded*).

> Meta: entender **cuÃ¡ndo conviene proyectar** (PCA) y **cuÃ¡ndo conviene elegir** (FS), y cÃ³mo eso impacta en **desempeÃ±o, interpretabilidad y robustez**.

---

# ğŸ¯ Objetivos

- Aplicar **PCA** (tras estandarizar) y evaluar el â€œcodoâ€ de varianza explicada.  
- Probar **Feature Selection** en tres sabores:  
  - *Filter*: `f_regression`, `mutual_info_regression`.  
  - *Wrapper*: `RFE` con estimador base.  
  - *Embedded*: `Lasso (L1)` y **Random Forest** (importancias).  
- **Comparar** con una mÃ©trica clara (RMSE, RÂ²) y **justificar** la elecciÃ³n final.  
- Redactar una **reflexiÃ³n** que conecte el resultado tÃ©cnico con necesidades de negocio.

---

# ğŸ“¦ Dataset

| Aspecto | DescripciÃ³n |
|---|---|
| **Fuente** | Kaggle â€” *House Prices: Advanced Regression Techniques* |
| **Tarea** | RegresiÃ³n (`SalePrice`) |
| **Filas/Columnas** | ~1460 Ã— ~80 (varÃ­a segÃºn versiÃ³n/limpieza) |
| **Tipos** | NumÃ©ricas y categÃ³ricas (muchas ordinales) |
| **Problemas tÃ­picos** | Valores faltantes, variables altamente correlacionadas, cardinalidad, escalas distintas |

---

# ğŸ§¹ Limpieza y preparaciÃ³n

Pasos mÃ­nimos esperados:
1. Eliminar `Id`.  
2. Separar `SalePrice` como `y` y el resto como `X`.  
3. **Imputar**: medianas para numÃ©ricas, mÃ¡s frecuente para categÃ³ricas.  
4. **Codificar** categÃ³ricas (rÃ¡pido: *Label Encoding*; ideal: One-Hot/Target Encoding segÃºn pipeline).  
5. **Estandarizar** para PCA / Lasso.

```python
import pandas as pd
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import LabelEncoder, StandardScaler

df = pd.read_csv("train.csv").drop(columns=["Id"])
y = df["SalePrice"].copy()
X = df.drop(columns=["SalePrice"]).copy()

num_cols = X.select_dtypes(include=["number"]).columns
cat_cols = X.select_dtypes(exclude=["number"]).columns

X[num_cols] = SimpleImputer(strategy="median").fit_transform(X[num_cols])
X[cat_cols] = SimpleImputer(strategy="most_frequent").fit_transform(X[cat_cols])

for c in cat_cols:
    X[c] = LabelEncoder().fit_transform(X[c].astype(str))

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)  # para PCA / Lasso
```

---

# ğŸ§ª DiseÃ±o experimental

- **ParticiÃ³n**: `train_test_split(test_size=0.2, random_state=42)`.  
- **Modelos de evaluaciÃ³n**:  
  - PCA + **LinearRegression** sobre PCs.  
  - FS (*filter*, *wrapper*, *embedded*) + **LinearRegression** (para aislar el efecto de la selecciÃ³n).  
- **MÃ©tricas**: **RMSE** (principal), **RÂ²** (secundaria).  
- **ComparaciÃ³n**: tabla final con el **mejor** resultado de cada bloque.

---

# ğŸ§© PCA (Dimensionalidad)

1) Ajustar PCA sobre `X_scaled`.  
2) Graficar **varianza explicada acumulada** y elegir `k` (90%â€“95%).  
3) Entrenar **LR** sobre las `k` PCs y evaluar.

```python
from sklearn.decomposition import PCA
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score
import numpy as np
import matplotlib.pyplot as plt

pca = PCA().fit(X_scaled)
cum = np.cumsum(pca.explained_variance_ratio_)

plt.plot(range(1, len(cum)+1), cum, marker="o")
plt.axhline(0.90, ls="--"); plt.axhline(0.95, ls="--")
plt.xlabel("n_components"); plt.ylabel("Explained Variance (cum)"); plt.show()

k = int(np.argmax(cum >= 0.90) + 1)

X_pca = PCA(n_components=k).fit_transform(X_scaled)
Xtr, Xte, ytr, yte = train_test_split(X_pca, y, test_size=0.2, random_state=42)

rmse = mean_squared_error(yte, LinearRegression().fit(Xtr, ytr).predict(Xte), squared=False)
```

---

# ğŸ·ï¸ Feature Selection

## 1) Filter (barato y rÃ¡pido)
- **F-test** (`f_regression`): relaciÃ³n lineal.  
- **Mutual information**: relaciones no lineales.

```python
from sklearn.feature_selection import SelectKBest, f_regression, mutual_info_regression

def run_filter(score_func, k):
    sel = SelectKBest(score_func=score_func, k=k).fit(X, y)
    cols = X.columns[sel.get_support()]
    return cols
```

## 2) Wrapper (RFE â€” mÃ¡s caro, mÃ¡s fino)
- `RFE` con **LinearRegression** o **RandomForestRegressor** como estimador base.  
- Costo: alto (iterativo). Beneficio: selecciÃ³n dirigida por el modelo.

```python
from sklearn.feature_selection import RFE
from sklearn.ensemble import RandomForestRegressor
RFE(LinearRegression(), n_features_to_select=20).fit(X, y)
```

## 3) Embedded
- **Lasso (L1)**: *shrinkage* a cero â†’ selecciÃ³n implÃ­cita (necesita escalado).  
- **Random Forest**: ranking por importancias (robusto a escalas y no linealidades).

```python
from sklearn.linear_model import Lasso

lasso = Lasso(alpha=1e-3, max_iter=20000, random_state=42).fit(X_scaled, y)
selected = [c for c, coef in zip(X.columns, lasso.coef_) if coef != 0]
```

---

# âš™ï¸ AnÃ¡lisis tÃ©cnico

- **PCA**: reduce dimensionalidad **mezclando** variables â†’ mejor **robustez** y menos colinealidad; **pierde interpretabilidad** (PCs).  
- **Filter**: simple/rÃ¡pido, buena primera pasada; puede **ignorar interacciones**.  
- **Wrapper**: mÃ¡s preciso para un modelo especÃ­fico; **costoso** y puede sobreajustar si no se cuida.  
- **Embedded**: balance entre costo y seÃ±al; Lasso da **sparsity** e interpretabilidad; RF capta **no linealidades**.

> Regla de oro rÃ¡pida:  
> - Â¿QuerÃ©s **explicabilidad**? â†’ **FS** (Lasso/RFE).  
> - Â¿Te preocupa **colinealidad y velocidad**? â†’ **PCA** con LR.  
> - Â¿Relaciones no lineales**? â†’ RF (importancias) como brÃºjula.

---

# ğŸ“Š Resultados y discusiÃ³n

> **Tabla final â€” mejores casos de cada bloque**

| MÃ©todo | k / Î± | Modelo | RMSE | RÂ² | Notas |
|---|---:|---|---:|---:|---|
| **PCA** | 38 | LR | 26 620 | 0.8859 | PCs â‰¥80 % var. (79.5 % retenida) |
| **Filter (MI)** | 38 | LR | 26 279 | 0.8891 | Captura relaciones no lineales, rÃ¡pido y sÃ³lido |
| **Wrapper (RFE-LR)** | 19 | LR | â€” | â€” | Refinado iterativo; costo alto, mejora marginal |
| **Embedded (Lasso)** | Î± = 1375.38 | LR | 26 083 | 0.8908 | Sparse + interpretable; mejor rendimiento global |
| **Embedded (RF)** | 38 | LR | 26 238 | 0.8894 | Importancias Ãºtiles para ranking no lineal |

> ğŸ **Gana Lasso con 41â†’38 features:**  
> **RMSE = 26 083**, **RÂ² = 0.8908**.  
> Lo elijo porque mantiene **interpretabilidad**, **baja dimensionalidad** y **supera a PCA** en un set con variables categÃ³ricas codificadas.

---

# ğŸ”— ConexiÃ³n con otras unidades

- **UT2:** La selecciÃ³n de features mostrÃ³ quÃ© variables son realmente confiables antes de modelar.  
- **UT4:** Se consolidÃ³ un pipeline reproducible, con pasos ordenados y sin fuga de datos.  
- **UT5:** Se evaluÃ³ el costo/beneficio entre complejidad computacional e interpretabilidad del modelo.

---

# ğŸ§© ReflexiÃ³n final

En mi caso, **Lasso** fue la tÃ©cnica que mejor equilibrÃ³ **rendimiento y explicabilidad**.  
LogrÃ³ un RMSE â‰ˆ 26 k con menos de la mitad de las variables originales, eliminando redundancias y estabilizando el modelo.  
El **PCA** redujo la dimensionalidad sin gran pÃ©rdida de precisiÃ³n, pero sacrificÃ³ interpretabilidad: las componentes no tienen sentido directo para negocio.  
Los mÃ©todos **filter** y **wrapper** ayudaron a entender la contribuciÃ³n individual de cada feature, aunque los wrappers implican un costo alto para mejoras mÃ­nimas.  

En un entorno productivo recomendarÃ­a **Lasso o MI** como enfoques base: rÃ¡pidos, reproducibles y fÃ¡ciles de justificar frente al cliente.  
CuidarÃ­a especialmente evitar **data leakage** (fitear PCA o selecciÃ³n dentro del pipeline) y monitorearÃ­a posibles **overfits** si se combinan con modelos complejos.

---

# ğŸ§° Stack tÃ©cnico

**Lenguaje:** Python  
**LibrerÃ­as:** Pandas Â· NumPy Â· Scikit-learn Â· Matplotlib/Seaborn  
**Conceptos:** PCA Â· Filter/Wrapper/Embedded FS Â· RMSE/RÂ² Â· EstÃ¡ndar de evaluaciÃ³n

### ğŸ“ [Notebook](../../../notebooks/UT3-3.ipynb)

---

# ğŸ“š Referencias

- GuÃ­a UT3-10 â€” PCA & Feature Selection (UCU-ID): <https://juanfkurucz.com/ucu-id/ut3/10-pca-feature-selection/>  
- Scikit-learn: PCA, SelectKBest, RFE, Lasso, RandomForest.  
- Kaggle â€” *House Prices: Advanced Regression Techniques*.
