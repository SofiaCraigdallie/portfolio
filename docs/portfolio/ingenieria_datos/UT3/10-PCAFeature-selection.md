---
title: "ğŸ” UT3 Â· PrÃ¡ctica 10 â€” PCA vs Feature Selection en Ames Housing"
date: 2025-11-09
---

# ğŸ” PCA vs Feature Selection en Ames Housing

---

# ğŸŒ Contexto

Esta prÃ¡ctica corresponde a la **Unidad TemÃ¡tica 3: Feature Engineering y SelecciÃ³n de Variables**, dentro del Portafolio de IngenierÃ­a de Datos.  
Se trabajÃ³ con el dataset **Ames Housing** (competencia *House Prices* de Kaggle), un clÃ¡sico problema de regresiÃ³n donde el objetivo es predecir `SalePrice`.  

El foco estuvo en comparar dos estrategias frente a la alta dimensionalidad:
- **ReducciÃ³n de dimensionalidad** mediante **PCA** (transformaciÃ³n de componentes principales).  
- **SelecciÃ³n de variables** (*Feature Selection*) en sus tres enfoques: *Filter*, *Wrapper* y *Embedded*.  

> La meta fue entender **cuÃ¡ndo conviene proyectar (PCA)** y **cuÃ¡ndo conviene elegir (FS)**, y cÃ³mo cada enfoque impacta en el **rendimiento, interpretabilidad y costo computacional**.

---

# ğŸ¯ Objetivos

- Aplicar **PCA** y determinar el nÃºmero Ã³ptimo de componentes segÃºn la varianza acumulada.  
- Implementar **mÃ©todos de selecciÃ³n de variables**:  
  - *Filter*: `f_regression`, `mutual_info_regression`.  
  - *Wrapper*: `RFE` con `LinearRegression` o `RandomForest`.  
  - *Embedded*: `Lasso (L1)` y `RandomForest` (feature importances).  
- Evaluar el desempeÃ±o (RMSE, RÂ²) de cada tÃ©cnica.  
- Reflexionar sobre el balance entre **precisiÃ³n, interpretabilidad y eficiencia**.

---

# ğŸ“¦ Dataset

| Aspecto | DescripciÃ³n |
|---|---|
| **Fuente** | Kaggle â€” *House Prices: Advanced Regression Techniques* |
| **Tarea** | RegresiÃ³n (`SalePrice`) |
| **Filas/Columnas** | ~1460 Ã— ~80 (varÃ­a segÃºn versiÃ³n/limpieza) |
| **Tipos** | NumÃ©ricas y categÃ³ricas (muchas ordinales) |
| **Problemas tÃ­picos** | Valores faltantes, variables altamente correlacionadas, cardinalidad, escalas distintas |

---

# ğŸ§¹ Limpieza y preparaciÃ³n

1. Se eliminÃ³ la columna `Id`.  
2. Se separÃ³ `SalePrice` como variable objetivo `y`.  
3. Se imputaron valores faltantes (mediana para numÃ©ricas, moda para categÃ³ricas).  
4. Se codificaron las variables categÃ³ricas con *Label Encoding*.  
5. Se escalaron las variables con `StandardScaler` para PCA y Lasso.  

Estos pasos garantizaron consistencia y comparabilidad entre los distintos mÃ©todos.

---

# ğŸ§© PCA â€” ReducciÃ³n de Dimensionalidad

El AnÃ¡lisis de Componentes Principales permite condensar la informaciÃ³n de mÃºltiples variables en un conjunto mÃ¡s pequeÃ±o de componentes que explican la mayor parte de la varianza.

![Varianza explicada y acumulada del PCA](../../../assets/img/visualizacion_var.png)

**Figura 1.** A la izquierda se observa el *Scree Plot* (caÃ­da de la varianza individual).  
A la derecha, la varianza acumulada muestra que con **â‰ˆ38 componentes** se conserva alrededor del **90%** de la informaciÃ³n original.  

> Se seleccionaron **38 componentes principales** como punto de corte, priorizando equilibrio entre informaciÃ³n retenida y simplicidad del modelo.

![Importancia de variables segÃºn PCA](../../../assets/img/imp_ft.png)

**Figura 2.** Principales variables que mÃ¡s peso aportan a las componentes del PCA.  
Las Ã¡reas habitables (`Gr Liv Area`, `TotRms AbvGrd`) y caracterÃ­sticas de sÃ³tano y baÃ±o son las que mÃ¡s influyen, mostrando que las dimensiones espaciales dominan la varianza global del conjunto.

---

# ğŸ·ï¸ Feature Selection â€” MÃ©todos comparativos

## 1ï¸âƒ£ Filter Methods

Los mÃ©todos *Filter* seleccionan variables segÃºn su relaciÃ³n estadÃ­stica con la variable objetivo, sin depender de un modelo especÃ­fico.

![Ranking de features por F-test](../../../assets/img/ftest.png)

**Figura 3.** Ranking de variables segÃºn el **F-test (ANOVA)**. Las variables con mayor relaciÃ³n lineal con `SalePrice` son `Overall Qual`, `Gr Liv Area`, `Garage Cars` y `Garage Area`.

![Ranking de features por Mutual Information](../../../assets/img/mutualinfo.png)

**Figura 4.** Ranking basado en **Mutual Information**, que captura dependencias no lineales. Aparecen variables de calidad (`Neighborhood`, `Bsmt Qual`, `Exter Qual`) que el F-test no detecta, mostrando su poder para relaciones mÃ¡s complejas.

---

## 2ï¸âƒ£ Wrapper â€” RFE

Los mÃ©todos *Wrapper* iteran sobre combinaciones de variables para identificar subconjuntos Ã³ptimos segÃºn el rendimiento del modelo base.

![Ranking de features seleccionadas por RFE](../../../assets/img/rfe.png)

**Figura 5.** Ranking de features retenidas por **RFE**. Variables como `Bsmt Full Bath`, `Foundation`, `Kitchen Qual` y `Bsmt Qual` fueron las Ãºltimas eliminadas, seÃ±alando su relevancia estructural.

> Si bien RFE logra buena precisiÃ³n, su costo computacional es alto (requiere mÃºltiples entrenamientos).

---

## 3ï¸âƒ£ Embedded â€” Lasso y Random Forest

Los mÃ©todos *Embedded* realizan la selecciÃ³n dentro del propio entrenamiento del modelo.

![Importancia de variables segÃºn Random Forest](../../../assets/img/rf.png)

**Figura 6.** Importancia de variables segÃºn un **Random Forest**.  
Se observa una marcada concentraciÃ³n de peso en `Overall Qual` y `Gr Liv Area`, confirmando su alto poder predictivo.

![Importancia de coeficientes del modelo Lasso](../../../assets/img/lasso.png)

**Figura 7.** Magnitud de los coeficientes del modelo **Lasso (L1)**.  
Lasso actÃºa como un filtro automÃ¡tico: mantiene solo las variables mÃ¡s relevantes (`Gr Liv Area`, `Overall Qual`, `Exter Qual`) y reduce a cero las redundantes, simplificando la interpretaciÃ³n.

---

# â±ï¸ ComparaciÃ³n de costos

![ComparaciÃ³n de tiempo entre mÃ©todos de selecciÃ³n](../../../assets/img/fs_vs_rfe.png)

**Figura 8.** Comparativa de tiempo de ejecuciÃ³n.  
El mÃ©todo **RFE** ofrece resultados precisos pero con un **costo 15Ã— mayor** que los mÃ©todos *Filter* o *Embedded* (como Lasso).  
En contextos de producciÃ³n, la elecciÃ³n depende del balance entre **tiempo disponible** y **necesidad de precisiÃ³n**.

---

# ğŸ“Š Resultados y discusiÃ³n

> **Tabla final â€” mejores casos de cada bloque**

| MÃ©todo | k / Î± | Modelo | RMSE | RÂ² | Notas |
|---|---:|---|---:|---:|---|
| **PCA** | 38 | LR | 26 620 | 0.8859 | PCs â‰¥80 % var. (79.5 % retenida) |
| **Filter (MI)** | 38 | LR | 26 279 | 0.8891 | Captura relaciones no lineales, rÃ¡pido y sÃ³lido |
| **Wrapper (RFE-LR)** | 19 | LR | â€” | â€” | Refinado iterativo; costo alto, mejora marginal |
| **Embedded (Lasso)** | Î± = 1375.38 | LR | 26 083 | 0.8908 | Sparse + interpretable; mejor rendimiento global |
| **Embedded (RF)** | 38 | LR | 26 238 | 0.8894 | Importancias Ãºtiles para ranking no lineal |

> ğŸ **Gana Lasso con 41â†’38 features:**  
> **RMSE = 26 083**, **RÂ² = 0.8908**.  
> Lo elijo porque mantiene **interpretabilidad**, **baja dimensionalidad** y **supera a PCA** en un set con variables categÃ³ricas codificadas.

---

# ğŸ”— ConexiÃ³n con otras unidades

- **UT2:** La selecciÃ³n de features mostrÃ³ quÃ© variables son realmente confiables antes de modelar.  
- **UT4:** Se consolidÃ³ un pipeline reproducible, con pasos ordenados y sin fuga de datos.  
- **UT5:** Se evaluÃ³ el costo/beneficio entre complejidad computacional e interpretabilidad del modelo.

---

# ğŸ§© ReflexiÃ³n final

El trabajo confirmÃ³ que **PCA y Feature Selection no son competidores, sino herramientas complementarias.**  
Mientras PCA prioriza **compresiÃ³n y estabilidad**, los mÃ©todos de selecciÃ³n permiten **explicabilidad y control del modelo**.  

El **Lasso** resultÃ³ la mejor opciÃ³n para el caso Ames: redujo la dimensionalidad de forma natural, manteniendo las variables mÃ¡s significativas y logrando el menor RMSE.  
En entornos productivos, recomendarÃ­a **Lasso o Mutual Information** como estrategias base, combinadas con un pipeline que prevenga fugas de datos y preserve interpretabilidad.

---

# ğŸ§° Stack tÃ©cnico

**Lenguaje:** Python  
**LibrerÃ­as:** Pandas Â· NumPy Â· Scikit-learn Â· Matplotlib/Seaborn  
**Conceptos:** PCA Â· Filter/Wrapper/Embedded FS Â· RMSE/RÂ² Â· EstÃ¡ndar de evaluaciÃ³n

---

# Evidencias

### ğŸ“ [Notebook](../../../notebooks/UT3-3.ipynb)

---

# ğŸ“š Referencias

- GuÃ­a UT3-10 â€” PCA & Feature Selection (UCU-ID): <https://juanfkurucz.com/ucu-id/ut3/10-pca-feature-selection/>  
- Scikit-learn: PCA, SelectKBest, RFE, Lasso, RandomForest.  
- Kaggle â€” *House Prices: Advanced Regression Techniques*.