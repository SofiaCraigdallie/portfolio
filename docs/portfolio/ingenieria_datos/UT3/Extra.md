---
title: "ğŸ§ª UT3 Â· 03â€‘1 â€” Miniâ€‘Assignment: Feature Selection robusta vs PCA (Ames Housing)"
date: 2025-11-09
---

# ğŸ§ª Feature Selection robusta vs PCA (Ames Housing)

> **Trabajo extra sin guÃ­a oficial.** Objetivo: diseÃ±ar y ejecutar un **experimento reproducible** que compare **reducciÃ³n por proyecciÃ³n (PCA)** vs **selecciÃ³n de variables** bajo **validaciÃ³n robusta**, incorporando **estabilidad** y **explicabilidad**.

---

# ğŸŒ Contexto

Ames tiene mÃ¡s de 80 variables que describen casas: desde metros cuadrados hasta materiales o calidad de construcciÃ³n.  
Modelar esto directamente genera **colinealidad** y **ruido**, asÃ­ que decidÃ­ probar dos enfoques para simplificar:

1. **PCA:** comprime informaciÃ³n en componentes no correlacionadas (pero menos interpretables).  
2. **Feature Selection:** elige un subconjunto de variables originales (mÃ¡s explicables, pero potencialmente redundantes).

El objetivo fue comparar **rendimiento (RÂ², RMSE)**, **costo computacional** y **explicabilidad**, bajo validaciÃ³n cruzada y sin fuga de datos (*data leakage*).

---

# ğŸ¯ Objetivos

- Armar un **pipeline** (preproceso âœ selecciÃ³n/proyecciÃ³n âœ modelo) con `scikitâ€‘learn`.
- Evaluar **PCA** con distintos niveles de varianza acumulada (80/90/95%).
- Evaluar **FS** en modos *filter*, *wrapper*, *embedded* con una **grilla simple**.
- Medir **estabilidad de selecciÃ³n** por *bootstrapping* (frecuencia de inclusiÃ³n).
- Analizar **importancias/permutation importance** y **colinealidad residual**.
- Redactar una **discusiÃ³n** que justifique el â€œmejorâ€ enfoque.

---

# âš™ï¸ DiseÃ±o del experimento

ArmÃ© un pipeline con `scikit-learn` que integra todo: imputaciÃ³n, escalado, codificaciÃ³n, reducciÃ³n y modelo.  
Esto garantiza que **cada transformaciÃ³n se entrene solo dentro del fold de validaciÃ³n**, evitando que el test â€œveaâ€ informaciÃ³n del entrenamiento.

El modelo base fue una **regresiÃ³n lineal**, ideal para medir el efecto directo de la reducciÃ³n dimensional.  
ProbÃ© distintos niveles de varianza retenida en PCA (70%, 80%, 90%, 95%, 99%) y medÃ­ **tiempo de entrenamiento, inferencia y error**.  

---

# ğŸ“ˆ Resultados visuales

![EvoluciÃ³n de la varianza explicada, error y tiempos en funciÃ³n del nÃºmero de componentes (PCA)](../../../assets/img/ut3_extra.png)

- **Varianza explicada:** la curva azul muestra que la informaciÃ³n del dataset se concentra rÃ¡pidamente; con unas **50 componentes** ya se captura cerca del **90 %** de la varianza.  
  MÃ¡s allÃ¡ de eso, los incrementos son mÃ­nimos, lo que marca el punto de rendimiento decreciente.  

- **RMSE (error):** las barras rojas son prÃ¡cticamente planas entre 30 y 70 componentes, lo que significa que **mÃ¡s componentes no mejoran el modelo**.  
  El error se mantiene estable alrededor de los **26 000 $**, muy similar al baseline completo.

- **Tiempo de entrenamiento:** crece de forma casi lineal con el nÃºmero de componentes.  
  El mejor equilibrio entre costo y precisiÃ³n se da en torno a **40â€“50 componentes**.

- **Tiempo de inferencia:** apenas varÃ­a; en general, PCA reduce un poco la latencia promedio por muestra, pero la ganancia es menor frente al costo de cÃ¡lculo inicial.

ğŸ“Œ En resumen, el **PCA al 90 % de varianza (~50 componentes)** resultÃ³ ser el **â€œsweet spotâ€**: mantiene precisiÃ³n, baja ruido y acelera el pipeline sin perder capacidad predictiva.

---

# ğŸ§  ComparaciÃ³n conceptual

Luego de analizar PCA, volvÃ­ a los mÃ©todos de **selecciÃ³n de variables**.  
En contraste, los *filters* (por correlaciÃ³n o informaciÃ³n mutua) y *wrappers* (como RFE) ofrecen **mÃ¡s interpretabilidad** pero a un costo computacional alto y con riesgo de seleccionar variables redundantes.  

El **Lasso**, en cambio, combinÃ³ lo mejor de ambos mundos: produce un modelo **sparse**, elimina pesos insignificantes y deja un subset compacto de features con buen desempeÃ±o (â‰ˆ0.88 de RÂ²).  

PCA gana en eficiencia y estabilidad; Lasso gana en explicabilidad y anÃ¡lisis posterior.  
Ambos reducen dimensionalidad, pero desde lÃ³gicas completamente distintas.

---

# ğŸ’¬ ReflexiÃ³n

Este trabajo me sirviÃ³ para entender que **reducir dimensionalidad no es solo una decisiÃ³n tÃ©cnica, sino tambiÃ©n comunicacional**.  
En proyectos donde la interpretaciÃ³n importa (por ejemplo, justificar quÃ© factores encarecen una casa), la selecciÃ³n de variables con Lasso tiene mÃ¡s valor.  
En cambio, si el objetivo es pura optimizaciÃ³n, **PCA@90 %** ofrece un pipeline mÃ¡s limpio y liviano, ideal para despliegue o producciÃ³n.

---

# ğŸ”— ConexiÃ³n con otras unidades

- **UT2**: calidad y sesgos â†’ quÃ© variables son confiables antes de seleccionar.  
- **UT4**: *pipelines* y despliegue â†’ congelar *preprocess + selector + modelo*.  
- **UT5**: mÃ©tricas de negocio â†’ Â¿interpretabilidad > +0.01 de RÂ²?

---

# ğŸ§© ReflexiÃ³n final

ElegirÃ­a **Lasso** como selector primario: balancea rendimiento y explicabilidad y me deja un set compacto y defendible. 

Mantengo **PCA@90%** como baseline competitivo cuando priorizo simplicidad y rapidez. En revisiÃ³n, confirmo que no hay leakage y reporto `media Â± std` del CV.

---

# ğŸ§° Stack tÃ©cnico

**Lenguaje:** Python  
**LibrerÃ­as:** Pandas Â· NumPy Â· Scikitâ€‘learn Â· Matplotlib  
**Conceptos:** PCA Â· Filter/Wrapper/Embedded Â· Bootstrap Stability Â· Permutation Importance Â· KFold(5)

---

# Evidencias

### ğŸ“ [Notebook](../../../notebooks/UT3-Extra.ipynb)

---

# ğŸ“š Referencias

- Scikitâ€‘learn: PCA, SelectKBest, RFE, Lasso, RandomForest, permutation_importance.  
- Domingos (2012). *A few useful things to know about ML*. CACM.  
- Kuhn & Johnson (2019). *Feature Engineering and Selection*.
