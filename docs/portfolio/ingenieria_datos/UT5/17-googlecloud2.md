---
title: "â˜ï¸ UT5 â€“ Actividad 17: Creating a Data Transformation Pipeline with Cloud Dataprep"
date: 2025-12-02
---

# UT5 â€“ Google Cloud Skills: *Creating a Data Transformation Pipeline with Cloud Dataprep*

> ğŸš€ Lab GSP430 â€” Creando un pipeline de transformaciÃ³n con Dataprep

---

# ğŸŒ Contexto

En esta actividad completÃ© el lab **â€œCreating a Data Transformation Pipeline with Cloud Dataprep (GSP430)â€**, un lab intermedio donde usÃ¡s **Cloud Dataprep / Alteryx Designer Cloud** para preparar, limpiar y transformar datos de forma visual.

El lab es un clÃ¡sico caso de e-commerce: datos de sesiones en BigQuery â†’ los preparo en Dataprep â†’ los vuelvo a dejar listos en BigQuery como tabla final para reporting. Ideal para entender cÃ³mo es un pipeline ETL visual en Google Cloud.

---

# ğŸ¯ Objetivos

- Usar la interfaz visual de Dataprep sin morir en el intento
- Conectar BigQuery como fuente y como destino
- Explorar calidad y estructura del dataset
- Limpiar datos y quitar ruido
- Crear columnas nuevas y enriquecer el dataset
- Ejecutar el pipeline en Dataflow (bajo el capÃ³)
- Dejar los datos listos en BigQuery para anÃ¡lisis

---

# ğŸ•’ Actividades realizadas

| Actividad                    | Tiempo | Resultado                              |
| ---------------------------- | :----: | -------------------------------------- |
| Habilitar Dataprep           |   10m  | Servicio listo y permisos configurados |
| Crear dataset en BigQuery    |   10m  | Dataset `ecommerce` + tabla raw creada |
| Conectar BigQuery a Dataprep |   10m  | Flow creado y dataset importado        |
| Explorar columnas            |   15m  | Tipos, calidad, nulos, distribuciones  |
| Limpieza                     |   15m  | Filtros + columnas removidas           |
| Enriquecimiento              |   15m  | Nuevas columnas calculadas             |
| Ejecutar job                 |   10m  | Tabla `revenue_reporting` generada     |

---

# ğŸ—ï¸ Desarrollo del Lab

## 1) HabilitaciÃ³n y acceso a Dataprep

Antes de usar Dataprep hay que habilitar el servicio y crear la identidad:

```bash
gcloud beta services identity create --service=dataprep.googleapis.com
```

Luego entrÃ© por:

**Console â†’ Analytics â†’ Alteryx Designer Cloud**

- AceptÃ¡s tÃ©rminos
- PermitÃ­s acceso al proyecto
- Te logueÃ¡s con la cuenta temporal
- Dataprep crea su bucket y listo

**Dato importante**: solo funciona bien en Chrome.

---

## 2) Dataset inicial en BigQuery

CreÃ© un dataset:

- **ID**: `ecommerce`
- UbicaciÃ³n default (US)

DespuÃ©s ejecutÃ© una query para traer un subset de datos reales de Google Analytics (solo un dÃ­a):

```sql
CREATE OR REPLACE TABLE ecommerce.all_sessions_raw_dataprep AS
SELECT *
FROM `data-to-insights.ecommerce.all_sessions_raw`
WHERE date = '20170801';
```

Quedan ~56.000 filas con datos reales de sessions:
visitorId, hits, revenue, pageviews, ciudades, etc.

---

## 3) Conectar BigQuery a Dataprep

En Dataprep:

1. **Create a new flow**

   * Name: *Ecommerce Analytics Pipeline*
   * Description: *Revenue reporting table*

2. Add dataset â†’ elegir BigQuery â†’ dataset `ecommerce` â†’ tabla `all_sessions_raw_dataprep`.

Dataprep analiza automÃ¡ticamente la estructura y calidad apenas cargas los datos.

---

## 4) ExploraciÃ³n visual

La interfaz de Dataprep es muy cÃ³moda para revisar el dataset:

- **Esquema** (izquierda): tipos detectados, longitudes, nulos
- **Datos sampleados** (centro): preview con colores segÃºn calidad
- **Sugerencias** (derecha): transformaciones recomendadas

Lo mejor:

- Histogramas por columna
- Detecta tipos errÃ³neos
- Marca problemas de calidad
- Identifica valores raros

EncontrÃ©:

- Campos numÃ©ricos tÃ­picos: revenue, pageviews
- Datos categÃ³ricos: city, country, SKU
- Varios nulos en columnas de transacciÃ³n
- Columna `eCommerceAction_type` codificada (0â€“8)

---

## 5) Limpieza de datos

### ğŸ”¹ Filtro de hits relevantes (PAGE)

El dataset tenÃ­a varios tipos de hit. Para anÃ¡lisis de pÃ¡ginas, solo me sirven los **PAGE**.

Pasos:

- Click en la barra â€œPAGEâ€ del histograma
- â€œKeep rowsâ€
- Listo, filtrado sin escribir cÃ³digo

### ğŸ”¹ EliminaciÃ³n de columnas inÃºtiles

RemovÃ­ varias columnas:

- Totalmente nulas
- Duplicadas
- Metadatos internos que no aportaban

---

## 6) Enriquecimiento del dataset

### ğŸ”¹ Crear `unique_session_id`

Los campos `fullVisitorId` y `visitId` por separado no identifican una sesiÃ³n globalmente.

SoluciÃ³n: concatenarlos.

Dataprep â†’ Merge columns

- Separador: `-`
- Nuevo campo: `unique_session_id`

### ğŸ”¹ Etiquetar acciones e-commerce

La columna `eCommerceAction_type` venÃ­a con nÃºmeros 0â€“8.

CreÃ© un **case statement** con etiquetas inteligibles:

- 0 â†’ Unknown
- 3 â†’ Add to cart
- 6 â†’ Completed purchase
  ... etc.

Nueva columna: `eCommerceAction_label`.

### ğŸ”¹ NormalizaciÃ³n de revenue

`totalTransactionRevenue` viene *multiplicado por un millÃ³n*.

TransformaciÃ³n:

```
DIVIDE(totalTransactionRevenue,1000000)
```

Nueva columna decimal: `totalTransactionRevenue1`.

---

## 7) EjecuciÃ³n del pipeline â†’ BigQuery

La ejecuciÃ³n se hace vÃ­a:

**Dataflow + BigQuery**

Pasos:

- Run Job â†’ Edit
- Output: BigQuery
- Dataset: `ecommerce`
- Nueva tabla: `revenue_reporting`
- OpciÃ³n: Overwrite (â€œDrop table every runâ€)
- RUN

Dataflow compila todo a Apache Beam y lo ejecuta en paralelo.

Luego verifiquÃ© la tabla final en BigQuery.

---

# ğŸ§  Conceptos clave aprendidos

### ğŸ”¸ Cloud Dataprep

- Herramienta visual, cero cÃ³digo
- DetecciÃ³n automÃ¡tica de calidad
- Sugerencias inteligentes
- IntegraciÃ³n total con Google Cloud

### ğŸ”¸ Flows & Recipes

- Flow = el pipeline entero
- Recipe = la lista de transformaciones
- Cada paso se puede ver/editar con preview en tiempo real

### ğŸ”¸ Transformaciones Ãºtiles

- Filtros
- EliminaciÃ³n de columnas
- Create column
- Case statements
- Merge columns
- Custom formulas

### ğŸ”¸ BigQuery â†” Dataprep

- BigQuery como entrada y salida
- Lectura rÃ¡pida de datos grandes
- Escritura escalable con Dataflow

### ğŸ”¸ Dataflow

Dataprep â†’ genera cÃ³digo Beam â†’ Dataflow lo ejecuta escalado.
Escala solo, maneja errores y paraleliza sin esfuerzo.

---

# ğŸš€ Aplicaciones prÃ¡cticas

### âœ”ï¸ ML

Ideal para preparar features antes de entrenar modelos (normalizaciÃ³n, limpieza, derivaciÃ³n).

### âœ”ï¸ ETL visual

Perfecto para analistas o equipos mixtos que no quieren escribir Python/Spark.

### âœ”ï¸ Reporting

Facilita la construcciÃ³n de tablas limpias para dashboards.

### âœ”ï¸ Data Quality

Te permite entender un dataset desconocido en minutos.

### âœ”ï¸ ColaboraciÃ³n

Documenta las transformaciones de forma visual â€”todo el equipo lo entiende.

---

# âš ï¸ DesafÃ­os

1. **Interfaz cargada**
   â†’ Se resuelve con prÃ¡ctica y entendiendo Flow â†’ Recipe.

2. **Solo Chrome**
   â†’ LimitaciÃ³n tÃ©cnica.

3. **Case statements largos**
   â†’ Requieren paciencia, pero Dataprep te deja ver todo antes de aplicar.

4. **Jobs lentos**
   â†’ Dataflow tarda. Normal en pipelines distribuidos.

5. **Debugging**
   â†’ Hacer transformaciones de a una ayuda muchÃ­simo.

---

# ğŸ“ ReflexiÃ³n final

Este lab es una excelente introducciÃ³n al enfoque â€œvisual-firstâ€ para ETL. Te muestra:

- Que Dataprep puede reemplazar mucho cÃ³digo para tareas repetitivas
- Que BigQuery y Dataprep se integran sin fricciÃ³n
- Que Dataflow te da escalabilidad real sin escribir Beam
- Que explorar datos visualmente acelera muchÃ­simo la comprensiÃ³n

**Takeaways clave**:

- ExplorÃ¡ antes de transformar
- Dataprep es fuerte en calidad de datos
- La integraciÃ³n BigQuery â†” Dataprep â†” Dataflow es muy poderosa
- Los recipes funcionan como documentaciÃ³n ejecutable
- Es una herramienta ideal para prototipar rÃ¡pido ETLs complejos

---

# ğŸ“š Referencias

- [Google Cloud Skills Boost](https://www.skills.google/)
- [Lab GSP430: Creating a Data Transformation Pipeline with Cloud Dataprep](https://www.skills.google/focuses/4415?catalog_rank=%7B%22rank%22%3A6%2C%22num_filters%22%3A1%2C%22has_search%22%3Atrue%7D&parent=catalog&search_id=60910456)
- [Cloud Dataprep Documentation](https://cloud.google.com/dataprep/docs)
- [Alteryx Designer Cloud](https://www.alteryx.com/products/alteryx-designer-cloud)
- [BigQuery Documentation](https://cloud.google.com/bigquery/docs)
- [Cloud Dataflow Documentation](https://cloud.google.com/dataflow/docs)