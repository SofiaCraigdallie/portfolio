---
title: "üß™ UT4 ¬∑ Control de Calidad de Audio"
date: 2025-12-02
---

# üìä UT4 ‚Äî Extra: Dashboard de Control de Calidad para Audio (QA)

---

# üåç Contexto

En la UT4 trabaj√© con audio como tipo de dato especial: preprocesamiento, extracci√≥n de MFCCs y an√°lisis b√°sico de se√±ales.  
En esta tarea extra quise dar un paso m√°s hacia una mirada de **MLOps / monitoreo de datos**, construyendo un **dashboard de calidad** para un subconjunto del dataset UrbanSound8K.

La idea central es simple: antes de entrenar modelos de clasificaci√≥n de audio, necesito saber **qu√© tan confiable es el dataset** que estoy alimentando al pipeline.

---

# üéØ Objetivos

**Objetivo general**

- Dise√±ar un **Dashboard de QA** para audios que permita identificar grabaciones problem√°ticas despu√©s del preprocesamiento est√°ndar.

**Objetivos espec√≠ficos**

- Estandarizar un lote de audios a misma duraci√≥n y sample rate.
- Calcular m√©tricas simples por archivo (duraci√≥n, energ√≠a, ‚Äúruidosidad‚Äù).
- Definir umbrales tipo **sem√°foro** (OK / DUDOSO / MALO).
- Resumir qu√© porcentaje del dataset cae en cada categor√≠a y discutir implicancias para modelado.

---

# üì¶ Dataset

- **Fuente**: [UrbanSound8K](https://www.kaggle.com/datasets/chrisfilo/urbansound8k)
- **Tipo**: clips de audio urbanos etiquetados (sirenas, bocinas, etc.).
- **Subset usado**: primeros **200 audios** encontrados en la estructura del dataset.
- **Formato**: archivos `.wav` (y otros), le√≠dos desde la ruta provista por `kagglehub`.

Cada archivo se procesa de forma independiente, pero usando exactamente el mismo pipeline de estandarizaci√≥n que en la pr√°ctica principal de la UT4.

---

# üßπ Limpieza y preprocesamiento

Se reutiliz√≥ el pipeline est√°ndar de la UT4:

- Conversi√≥n a **mono** (promedio de canales).
- **Trim de silencios**: `librosa.effects.trim(y, top_db=30.0)`.
- **Resampleo** a `TARGET_SR = 16000 Hz`.
- **Recorte / padding** a `TARGET_DURATION = 3.0 s`.
- **Normalizaci√≥n de amplitud** a `TARGET_AMPLITUDE = 0.99`.

Despu√©s de este pipeline, todos los audios quedan:

- Con la misma **duraci√≥n nominal** (3.0 s).
- Con el mismo **sample rate**.
- Con amplitud normalizada para evitar clipping.

---

# üîç EDA (exploraci√≥n de calidad)

Para cada uno de los ~200 audios estandarizados se calcularon las siguientes m√©tricas:

- `duration_sec`: duraci√≥n efectiva en segundos.
- `rms_mean`: energ√≠a promedio de la se√±al (Root Mean Square).
- `zcr_mean`: tasa de cruces por cero, usada como proxy de **ruido / transitorios**.

Se generaron histogramas para cada m√©trica:

## Histogramas de m√©tricas

![Histogramas de m√©tricas de calidad](../../../assets/img/histogramasaudio.png)

Los histogramas muestran tres cosas importantes:
- La duraci√≥n est√° perfectamente controlada (todos los audios quedan en 3.0 s).
- La energ√≠a RMS es muy variable: hay se√±ales extremadamente d√©biles y otras excesivamente fuertes.
- La ZCR muestra una cola larga hacia la derecha, evidenciando audios ruidosos o con muchos transitorios.

Esta exploraci√≥n ya sugiere que, aunque el pipeline es consistente, el **dataset en s√≠ es muy heterog√©neo**.

---

# ‚öôÔ∏è An√°lisis t√©cnico

## 1. Sem√°foro de calidad

Se definieron umbrales tipo sem√°foro:

- **Duraci√≥n**
  - OK: |dur ‚àí TARGET_DURATION| ‚â§ 0.3 s  
  - DUDOSO: hasta el doble de esa tolerancia  
  - MALO: desv√≠os todav√≠a mayores  

- **RMS** y **ZCR**
  - Se usaron **cuantiles 20% y 80%** para definir rangos ‚Äúnormales‚Äù.
  - Valores fuera de `[q20, q80]` ‚Üí MALO.
  - Valores cerca de los bordes ‚Üí DUDOSO.
  - Resto ‚Üí OK.

Luego se defini√≥ un **flag global por archivo**:

- Si alguna m√©trica est√° en **MALO** ‚Üí `global_flag = MALO`.
- Si no hay MALO pero hay al menos un **DUDOSO** ‚Üí `global_flag = DUDOSO`.
- Si todo es OK ‚Üí `global_flag = OK`.

## 2. Resultados num√©ricos

Sobre los ~200 audios analizados se obtuvo:

## Resumen global de calidad

![Global flags](../../../assets/img/global.png)

El sem√°foro global resume de forma simple la calidad del lote:
- Aproximadamente **1 de cada 5** audios est√° en condiciones √≥ptimas.
- Un **16%** requiere revisi√≥n (se√±al dudosa).
- Un **62%** presenta problemas claros de ruido o energ√≠a.

Este tipo de panel es √∫til para decidir si conviene filtrar, regrabar o aplicar preprocesamientos m√°s agresivos.

La gran mayor√≠a de problemas vienen de:

- RMS muy bajo (audios muy d√©biles).
- RMS muy alto (posible clipping).
- ZCR muy alta (ruido fuerte o muchos transitorios).

## 3. Nota √©tica / de dise√±o de datos

Usar un sem√°foro estricto tiene ventajas y riesgos:

- **Ventaja**: detecta f√°cilmente audios potencialmente inutilizables o claramente defectuosos.
- **Riesgo**: si los criterios son muy agresivos, se puede descartar material valioso, sesgar el dataset hacia ciertas condiciones de grabaci√≥n y reducir la diversidad de escenarios.

En un sistema productivo, estos umbrales deber√≠an:

- Estar **documentados**,  
- Poder **ajustarse**,  
- Y combinarse con **revisi√≥n humana** al menos en una muestra.

---

# üß† Resultados y discusi√≥n

Los resultados del dashboard muestran que:

1. **El pipeline de preprocesamiento funciona correctamente** en t√©rminos de duraci√≥n y sample rate: todos los audios quedan con 3.0 s y condiciones homog√©neas de muestreo.
2. A pesar de esto, el dataset presenta una **variabilidad enorme** en energ√≠a y ruido:
   - Muchos audios son demasiado d√©biles o demasiado fuertes.
   - Otros parecen extremadamente ruidosos seg√∫n la ZCR.
3. M√°s de la mitad de los audios quedan marcados como **MALO** bajo un criterio estricto.

Esto sugiere que, antes de entrenar un modelo de clasificaci√≥n de audio, es importante:

- Revisar al menos una parte de esos audios ‚Äúmalos‚Äù.
- Considerar regrabaciones, filtrados adicionales o exclusi√≥n selectiva.
- Documentar la **pol√≠tica de QA** que se aplica al dataset.

El dashboard no resuelve el problema, pero da una **foto r√°pida del estado del corpus**, que es exactamente lo que se busca en una etapa de QA.

---

# üîó Conexi√≥n con otras unidades

- **UT1‚ÄìUT2 (Data pipelines / IO y limpieza):**  
  Aqu√≠ se retoma la idea de construir **pipelines reproducibles**, pero aplicada a audio.  
  El dashboard se podr√≠a automatizar como un paso m√°s del pipeline de ingesti√≥n.

- **UT3 (Feature Engineering y validaci√≥n):**  
  La calidad de las features MFCC depende fuertemente de la calidad de la se√±al.  
  Este QA act√∫a como una **capa previa** a la ingenier√≠a de features.

- **UT4 (Datos especiales ‚Äì audio):**  
  Se conecta directamente con la pr√°ctica de MFCC y preprocesamiento, pero ahora con foco en **monitorear calidad**, no solo en extraer features.

---

# üß© Reflexi√≥n final

Esta tarea extra me sirvi√≥ para:

- Ver que **no alcanza con ‚Äútener el dataset‚Äù**: hay que mirarlo cr√≠ticamente.
- Experimentar con m√©tricas muy simples (duraci√≥n, RMS, ZCR) que, sin embargo, dan informaci√≥n poderosa sobre la salud del corpus.
- Pensar el audio desde la perspectiva de **calidad de datos**, no solo como insumo para un modelo.

La conclusi√≥n principal es que un pipeline de audio serio deber√≠a incluir alg√∫n tipo de **Dashboard de QA** como el que constru√≠ ac√°, para evitar entrenar modelos sobre datos ruidosos o directamente defectuosos.

---

# üß∞ Stack t√©cnico

- **Lenguaje**: Python
- **Librer√≠as principales**:
  - `librosa` (carga de audio, MFCC, ZCR, RMS, trim)
  - `numpy`, `pandas`
  - `matplotlib` (visualizaciones)
  - `kagglehub` (descarga del dataset UrbanSound8K)
- **Dataset**: UrbanSound8K (subset de 200 audios)
- **Pipeline propio**:
  - `preprocess_audio` (mono, trim, resampleo, duraci√≥n fija, normalizaci√≥n)
  - C√°lculo de m√©tricas por archivo y flags de calidad.

---

# Evidencias

### üìù [Notebook](../../../notebooks/UT4-extra-audio-qa.ipynb)

---

# üìö Referencias

- UrbanSound8K dataset ‚Äî Kaggle.  
- Librosa documentation: audio loading, feature extraction, effects.  
- Material de la UT4 ‚Äì Audio como dato (preprocesamiento y extracci√≥n de MFCCs).